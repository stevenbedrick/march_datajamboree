{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python: A Crash Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scott Chamberlain has put together [a version of this workflow in R](https://github.com/sckott/talks/blob/gh-pages/oddpdx/code/demo1.Rmd). The basic p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our Goal: Get the complete list of species in Hoyt Arboretum\n",
    "\n",
    "The Hoyt Arboretum maintains a database of the various species of tree, along with their locations in the garden, provenance information, etc. Unfortunately, this database is designed for use by humans- not computers. The list of species is broken out by letter of the alphabet...\n",
    "\n",
    "![Hoyt Index Page](hoyt_index.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And each letter has its own index:\n",
    "\n",
    "![Hoyt Index Page](hoyt_a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually visit each letter's index page, copy-and-paste the list into a text editor, and go from there... but that would get very old very fast! Let's write a script to automte the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping: The general process\n",
    "\n",
    "We will actually do this in three steps. First, we will get a list of all of the letter-specific index pages. Then, we will write a function to visit a letter index page, extract all of the species names, and return them as a Python list. Finally, we will put it all together in a script and write the output as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Libraries we will use:\n",
    "\n",
    "Web scraping in Python is made easier by a couple of libraries:\n",
    "\n",
    "`requests`: To download the contents of a web page\n",
    "\n",
    "`lxml`: To allow us to process the contents of a webpage\n",
    "\n",
    "An optional library that you will also want to have installed (but do not need to explicitly import) is `cssselect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting the list of letter-index page URLs\n",
    "\n",
    "### First: Download the main index page and turn it into something useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_index_page = requests.get(\"https://hoytarboretum.gardenexplorer.org/taxalist.aspx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_index_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_index_tree = html.fromstring(main_index_page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, figure out which HTML elements we care about:\n",
    "\n",
    "Firefox, Chrome, and Safari all have a special mode for viewing the source code to a web page that you are looking at, and this mode has useful tools for analyzing that source code. One particularly helpful tool lets you click on an element in the page and see exactly what code produced it. This is very helpful for reverse-engineering a web page for scraping. On Safari, you bring this mode up by pressing Command-Option-I.\n",
    "\n",
    "![Hoyt Index Page web inspector](hoyt_index_inspector.png)\n",
    "\n",
    "Looking at this, we can see that the letter buttons are all children of a div with the \"content\" class. \n",
    "\n",
    "We can write a \"CSS Selector\" to grab those tags for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_index_links = main_index_tree.cssselect(\"div.content a\") # \"a\" elements that are the children of a div of class \"content\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if that got what we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letter_index_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! This was a nice and simple one; often, writing the right CSS selector is a bit trickier. Some styles of writing HTML result in easier-to-scrape pages than others. Trial-and-error is usually a part of the process!\n",
    "\n",
    "What have we actually gotten, though? We have a list of `Element` objects, each of which represents one of the `a` tags in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element a at 0x1127a2ea8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_index_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ctl00_ContentPlaceHolder1_Repeater1_ctl00_HyperLink1', 'title': ' A ', 'href': 'taxalist-A.aspx'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_index_links[0].attrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once we've got the right selector set up, and have the elements we care about, it's time to pull whatever information out of them that we are after.\n",
    "\n",
    "### Third, actually extract information\n",
    "\n",
    "So, what have we captured? We have captured a list of objects representing HTML elements. In our case, they are all links (\"anchor\" tags), and we want to grab their destinations- represented by their `href` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_index_urls = [a.get(\"href\") for a in letter_index_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taxalist-A.aspx',\n",
       " 'taxalist-B.aspx',\n",
       " 'taxalist-C.aspx',\n",
       " 'taxalist-D.aspx',\n",
       " 'taxalist-E.aspx',\n",
       " 'taxalist-F.aspx',\n",
       " 'taxalist-G.aspx',\n",
       " 'taxalist-H.aspx',\n",
       " 'taxalist-I.aspx',\n",
       " 'taxalist-J.aspx',\n",
       " 'taxalist-K.aspx',\n",
       " 'taxalist-L.aspx',\n",
       " 'taxalist-M.aspx',\n",
       " 'taxalist-N.aspx',\n",
       " 'taxalist-O.aspx',\n",
       " 'taxalist-P.aspx',\n",
       " 'taxalist-Q.aspx',\n",
       " 'taxalist-R.aspx',\n",
       " 'taxalist-S.aspx',\n",
       " 'taxalist-T.aspx',\n",
       " 'taxalist-U.aspx',\n",
       " 'taxalist-V.aspx',\n",
       " 'taxalist-W.aspx',\n",
       " 'taxalist-X.aspx',\n",
       " 'taxalist-Y.aspx',\n",
       " 'taxalist-Z.aspx']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_index_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Processing a letter-index page\n",
    "\n",
    "Now we're ready to look at the letter-index pages. We will follow the same procedure for these as we did for the main index: look at the HTML, figure out a selector that works, and go from there.\n",
    "\n",
    "![Hoyt Index Letter Page web inspector](hoyt_a_inspector.png)\n",
    "\n",
    "We can see that the information we want is in an unordered list (`ul`) with the class of \"taxalist\", that each species has its own list item (`li`), and that each entry in the list contains a link with both the species name as well as some other information- the common name, an icon showing whether a photo is available, etc. The species name is in boldface (`b`). So, our selector should be something like:\n",
    "\n",
    "    ul.taxalist li span a b\n",
    "   \n",
    "Note that this is not the most parsimonious selector that would work. There are often multiple valid ways to design a selector...\n",
    "\n",
    "Also, note that it is not _always_ true that the latin name is in boldface- there are several plants on the \"A\" page that, for whatever reason, are italicized instead. So we will have to be a bit more creative... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_letter_index(url):\n",
    "    page = requests.get(\"https://hoytarboretum.gardenexplorer.org/{}\".format(url))\n",
    "    # nb: we should be doing some error checking here, to make sure that the request was successful, etc.\n",
    "    tree = html.fromstring(page.content) \n",
    "    for a in tree.cssselect(\"ul.taxalist li span a\"):\n",
    "        # whatever the child of this a tag is (whether b or i), let's get it and return its text:\n",
    "        yield a.getchildren()[0].text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abies alba',\n",
       " \"Abies alba 'Argau'\",\n",
       " \"Abies alba 'Badenweiler'\",\n",
       " \"Abies alba 'Green Spiral'\",\n",
       " \"Abies alba 'Pendula'\",\n",
       " 'Abies amabilis',\n",
       " 'Abies balsamea',\n",
       " \"Abies balsamea 'Nana'\",\n",
       " 'Abies balsamea var. phanerolepis',\n",
       " 'Abies bracteata',\n",
       " 'Abies cephalonica',\n",
       " \"Abies cephalonica 'Meyer's Dwarf'\",\n",
       " 'Abies concolor',\n",
       " \"Abies concolor 'Conica'\",\n",
       " \"Abies concolor 'Winter Gold'\"]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(process_letter_index(\"taxalist-A.aspx\"))[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good! But what if we wanted not just the names of the species, but also the common names and the links to each one's page? That would be a little bit more complicated, because of how the HTML of the page is set up. Each link looks like this:\n",
    "\n",
    "    <li class=\"taxalist\" id=\"Taxon-312\">\n",
    "        <span dir=\"ltr\">\n",
    "            <a id=\"ctl00_ContentPlaceHolder1_Repeater2_ctl01_HyperLink2\" href=\"taxon-312.aspx\">\n",
    "                <b>Abies alba</b>\n",
    "            </a>\n",
    "        </span>\t\n",
    "        <span id=\"ctl00_ContentPlaceHolder1_Repeater2_ctl01_TaxonDetails\" class=\"textmedium colorlabel\">\n",
    "            Pinaceae • European Silver Fir\n",
    "        </span>\n",
    "        <img id=\"ctl00_ContentPlaceHolder1_Repeater2_ctl01_hasImage\" title=\"Has image\" src=\"Images/photo.png\" alt=\"Has image\" style=\"height:10px;width:12px;border-width:0px;\">\n",
    "    </li>\n",
    "    \n",
    "What we would have to do would be to do additional processing to each `<li>` element: first grabbing the link, extracting its `href`, and then grabbing its child `b` node. Then, we'd need to do a second step of processing of the span containing the common name, then parse out _its_ contents. It can get a little tedious..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_letter_index_detail(url):\n",
    "    page = requests.get(\"https://hoytarboretum.gardenexplorer.org/{}\".format(url))\n",
    "    # nb: we should be doing some error checking here, to make sure that the request was successful, etc.\n",
    "    tree = html.fromstring(page.content) \n",
    "    species = tree.cssselect(\"ul.taxalist li\")\n",
    "    for s in species:\n",
    "        detail_link = s.cssselect(\"a\")[0] # there's always only one\n",
    "        link_dest = detail_link.get(\"href\")\n",
    "        \n",
    "        latin_name_a = s.cssselect(\"span a\")[0]\n",
    "        latin_name = latin_name_a.getchildren()[0].text.strip() # ditto\n",
    "            \n",
    "        # exercise for the reader: extend this script to also grab family and common names\n",
    "        \n",
    "        yield {'latin_name': latin_name, 'url': link_dest}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'latin_name': 'Zanthoxylum aff. diacanthoides', 'url': 'taxon-1345.aspx'},\n",
       " {'latin_name': 'Zanthoxylum americanum', 'url': 'taxon-977.aspx'},\n",
       " {'latin_name': 'Zanthoxylum nepalense', 'url': 'taxon-1742.aspx'},\n",
       " {'latin_name': 'Zanthoxylum piperitum', 'url': 'taxon-1263.aspx'},\n",
       " {'latin_name': 'Zanthoxylum planispinum', 'url': 'taxon-978.aspx'},\n",
       " {'latin_name': 'Zanthoxylum schinifolium', 'url': 'taxon-1743.aspx'},\n",
       " {'latin_name': 'Zanthoxylum simulans', 'url': 'taxon-1149.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Calistoga'\",\n",
       "  'url': 'taxon-1921.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Carmen's Gray'\",\n",
       "  'url': 'taxon-1922.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Dublin'\", 'url': 'taxon-1923.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Silver Select'\",\n",
       "  'url': 'taxon-1925.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Solidarity Pink'\",\n",
       "  'url': 'taxon-1444.aspx'},\n",
       " {'latin_name': \"Zauschneria californica 'Wayne's Silver'\",\n",
       "  'url': 'taxon-1434.aspx'},\n",
       " {'latin_name': 'Zelkova serrata', 'url': 'taxon-229.aspx'},\n",
       " {'latin_name': \"Zelkova serrata 'Goshiri'\", 'url': 'taxon-1966.aspx'},\n",
       " {'latin_name': \"Zelkova serrata 'Musashino'\", 'url': 'taxon-2302.aspx'},\n",
       " {'latin_name': \"Zelkova serrata 'Variegata'\", 'url': 'taxon-83.aspx'},\n",
       " {'latin_name': 'Zelkova sinica', 'url': 'taxon-82.aspx'}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(process_letter_index_detail(\"taxalist-Z.aspx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that this code is quite brittle, and makes many assumptions about the format of the HTML that it is processing. If the authors of the Hoyt Arboretum site change anything, it will almost certainly break our script. This is (sadly) a very common state of affairs. In fact, it is extremely rare for a scraping script to be reusable across multiple sites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('species_names.csv', 'w') as outfile:\n",
    "    fieldnames = ['latin_name', 'url']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for url in letter_index_urls:\n",
    "        for plant in process_letter_index_detail(url):\n",
    "            writer.writerow(plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin_name,url\r",
      "\r\n",
      "Abies alba,taxon-312.aspx\r",
      "\r\n",
      "Abies alba 'Argau',taxon-3693.aspx\r",
      "\r\n",
      "Abies alba 'Badenweiler',taxon-3699.aspx\r",
      "\r\n",
      "Abies alba 'Green Spiral',taxon-3698.aspx\r",
      "\r\n",
      "Abies alba 'Pendula',taxon-945.aspx\r",
      "\r\n",
      "Abies amabilis,taxon-316.aspx\r",
      "\r\n",
      "Abies balsamea,taxon-726.aspx\r",
      "\r\n",
      "Abies balsamea 'Nana',taxon-810.aspx\r",
      "\r\n",
      "Abies balsamea var. phanerolepis,taxon-1738.aspx\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head species_names.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zauschneria californica 'Carmen's Gray',taxon-1922.aspx\r",
      "\r\n",
      "Zauschneria californica 'Dublin',taxon-1923.aspx\r",
      "\r\n",
      "Zauschneria californica 'Silver Select',taxon-1925.aspx\r",
      "\r\n",
      "Zauschneria californica 'Solidarity Pink',taxon-1444.aspx\r",
      "\r\n",
      "Zauschneria californica 'Wayne's Silver',taxon-1434.aspx\r",
      "\r\n",
      "Zelkova serrata,taxon-229.aspx\r",
      "\r\n",
      "Zelkova serrata 'Goshiri',taxon-1966.aspx\r",
      "\r\n",
      "Zelkova serrata 'Musashino',taxon-2302.aspx\r",
      "\r\n",
      "Zelkova serrata 'Variegata',taxon-83.aspx\r",
      "\r\n",
      "Zelkova sinica,taxon-82.aspx\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! tail species_names.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! You can easily imagine how you might write a function to scrape data from the taxon subpages, and so forth. \n",
    "\n",
    "## In closing...\n",
    "\n",
    "While the specific CSS selectors that you construct will vary widely from page to page, the basic steps we've outlined here are remarkably consistent across scraping projects. Most programming languages and libraries have similar functionality to the ones found in Python's `lxml` and `requests` libraries, and CSS selectors are the same across languages. There are other ways to process HTML beyond CSS selectors- you may encounter scraping/parsing libraries that prefer XPath to CSS, for example. But again, the basic steps will be very similar to what we've outlined here.\n",
    "\n",
    "One thing to keep in mind: when scraping a large site (i.e., making hundreds-thousands of requests, etc.), it is considered polite to build some delay into your script, so as to avoid overloading the website. So, for example, in our loop above over each sub-page, we could have put a small random delay into each iteration, just to give the Hoyt Arboretum's server a break. And, of course, if a site's terms of use forbid automated scraping (as is the case with many scientific publishers' websites, for example), you should probably honor their requests. The case of Aaron Swartz and JSTOR is illustrative of how seriously some publishers take such matters.\n",
    "\n",
    "With that said, web scraping is a useful part of your programming and \"data science\" toolbox. Happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Bonus Content: Saving your data as JSON\n",
    "\n",
    "Above, we demonstrated how to save our scraped data as a CSV file. While CSV is a very important and useful format, depending on what your data look like and what you plan on doing with them downstream, you may wish to use something different. For example, if your data is anything other than simple tabular data, storing your data as serialized JSON objects may well be a better choice. Similarly, if all you're going to be doing is reading your data back in to Python (or some other similar language) for further analysis, using JSON over CSV _may_ save you some coding, depending on what your data look like.\n",
    "\n",
    "What is JSON? JSON stands for \"JavaScript Object Notation\", and it is a simple format for representing structured data (a list of strings, a dictionary of key-value pairs, etc.) as text. There are many ways to this, but JSON is particularly popular and useful for several reasons:\n",
    "\n",
    "1. It is human-readable, meaning that it is possible to examine JSON-encoded data with a text editor\n",
    "2. Its syntax is very simple, so writing a program to read or write JSON-encoded data is (relatively) straightforward\n",
    "3. Its syntax looks a lot like JavaScript, Python, Ruby, etc., meaning that it is very easy for programmers who are used to those languages to read (in fact, its syntax is almost-but-not-exactly identical to JavaScript- nearly all valid JSON objects are also valid JavaScript objects).\n",
    "4. Using JSON can _often_ be more reliable than CSV when working with non-Latin characters in your data (e.g. ∆, π, and 😁). \n",
    "\n",
    "Most languages have built-in functionality for working with JSON- both reading and writing. We can save our scraped Arboretum data in a couple of ways, but the simplest way is to just write it out to a file, one line per object. This style/method is sometimes called [\"JSON Lines\"](http://jsonlines.org), and is very convenient to work with.\n",
    "\n",
    "It is worth noting that, in the present example (i.e., Hoyt Arboretum's list of species), it is probably overkill to be using JSON. The main reason for doing so would be if we had some piece of software that we were planning on using downstream that specifically required/preferred JSON. Of course, if our data were more complex (for example, if we were to extend our scraper to visit each species' sub-page and pull out its geolocation, for exmple), JSON would be a very natural way to serialize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"species_list.json\",\"w\") as outfile:\n",
    "    for url in letter_index_urls:\n",
    "        for plant in process_letter_index_detail(url):\n",
    "            json.dump(plant, outfile, ensure_ascii=False) # in case of Unicode characters\n",
    "            outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in JSON-encoded data is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latin_name': 'Abies alba', 'url': 'taxon-312.aspx'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [json.loads(l) for l in open(\"species_list.json\")]\n",
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taxon-312.aspx'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_detail_urls = [s['url'] for s in names]\n",
    "species_detail_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way some people prefer to work with JSON-serialized data is to write it out as one giant object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "plants = {'species': [] }\n",
    "\n",
    "for url in letter_index_urls:\n",
    "    for this_species in process_letter_index_detail(url):\n",
    "        plants['species'].append(this_species)\n",
    "\n",
    "with open(\"species_list2.json\",\"w\") as outfile:\n",
    "    json.dump(plants, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plants_from_json = json.load(open(\"species_list2.json\"))\n",
    "len(plants_from_json['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you are! We have successfully round-tripped data from Python, out to JSON, and then back into Python. Most programming languages have similar JSON-handling functionality- though the exact functions you use might be different, the overall pattern will be similar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
